{% extends "base.html" %}

{% block content %}
    <div class="max-w-5xl mx-auto py-5 max-sm:p-4">
        <h1 class="max-sm:text-4xl max-sm:my-6 text-5xl my-12">A Comprehensive Guide to <span class="text-primary font-bold font-poppins">Sentiment Analysis.</span></h1>
        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Introduction</h2>
        <p class="max-sm:text-lg text-xl">Sentiment analysis (or opinion mining) is a technique that helps computers understand how people feel about something written in text â€” whether it's happy, sad, angry, or neutral. It is widely used in various fields, including customer feedback analysis, brand reputation management, social media monitoring, and market research.
            This guide will cover everything you need to know about sentiment analysis, from its basic concepts to advanced deep-learning techniques, including industry-standard approaches.</p>

        <h1 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">What is Sentiment Analysis?</h1>
        <p class="max-sm:text-lg text-xl">Sentiment analysis involves figuring out whether a sentence or paragraph is saying something positive, negative, neutral, or even expressing emotions like joy, anger, or sadness. It helps businesses, researchers, and developers understand public opinion, monitor trends, and automate decision-making processes.</p>

        <h1 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Types of Sentiment Analysis</h1>
        <ul class="max-sm:text-lg text-xl">
            <li><strong class="">Binary Sentiment Analysis: </strong>The simplest form where sentiment is classified as either positive or negative.</li>
            <li><strong class="">Ternary Sentiment Analysis: </strong>Classification into positive, negative, or neutral.</li>
            <li><strong class="">Fine-grained Sentiment Analysis: </strong>Classifies sentiment on a more granular level, such as - Very Positive, Positive, Neutral, Negative, Very Negative</li>
            <li><strong class="">Aspect-based Sentiment Analysis (ABSA): </strong>Identifies sentiment regarding specific aspects of a text.</li>
            <li><strong class="">Emotion Detection: </strong>Detects specific emotions like joy, anger, sadness, or fear.</li>
        </ul>

        <h1 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">How Sentiment Analysis Works?</h1>
        <p class="max-sm:text-lg text-xl">There are many ways to do sentiment analysis. Some are very simple, like checking for positive or negative words. Others use powerful AI models trained on large amounts of data.</p>

        <h2 class="max-sm:text-lg text-xl text-secondary font-bold">A. Rule-Based Approach</h2>
        <p class="max-sm:text-lg text-xl">Uses predefined rules and lexicons (word dictionaries) to classify sentiment.</p>
        <p class="max-sm:text-lg text-xl"><strong>Example:</strong> If a text contains words like "excellent", "good", or "amazing", it is classified as positive.</p>

        <h3 class="max-sm:text-lg text-xl font-bold">Popular lexicons:</h3>
        <ul class="max-sm:text-lg text-xl">
            <li> - AFINN</li>
            <li> - SentiWordNet</li>
            <li> - VADER (Valence Aware Dictionary and sEntiment Reasoner) (works well on social media text)</li>
        </ul>

        <h2 class="max-sm:text-lg text-xl text-secondary font-bold">B. Machine Learning Approach</h2>
        <p class="max-sm:text-lg text-xl">Uses labeled datasets to train models that predict sentiment. Common techniques include:</p>
        <ul class="max-sm:text-lg text-xl">
            <li> - Naive Bayes Classifier (Simple yet effective for text classification)</li>
            <li> - Support Vector Machines (SVMs)</li>
            <li> - Logistic Regression</li>
            <li> - Decision Trees and Random Forests</li>
        </ul>

        <h2 class="max-sm:text-lg text-xl text-secondary font-bold">C. Deep Learning Approach</h2>
        <ul class="max-sm:text-lg text-xl">
            <li><strong>Recurrent Neural Networks (RNNs) & Long Short-Term Memory (LSTMs)</strong>
                <ul class="max-sm:text-lg text-xl">
                    <li> - Used for sequential data like text.</li>
                    <li> - Can capture context better than traditional ML models.</li>
                </ul>
            </li>
            <li><strong>Transformers (BERT, RoBERTa, GPT)</strong>
                <ul class="max-sm:text-lg text-xl">
                    <li> - State-of-the-art NLP models.</li>
                    <li> - Pre-trained on large datasets for contextual understanding.</li>
                </ul>
            </li>
        </ul>

        <h1 class="max-sm:text-4xl text-5xl my-12">About <span class="text-primary font-bold font-poppins">DistilBERT-Base-Uncased-Finetuned-SST-2-English</span> Model</h1>
        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Introduction</h2>
        <p class="max-sm:text-lg text-xl"><strong>distilbert-base-uncased-finetuned-sst-2-english</strong> is a pretrained transformer-based model developed by Hugging Face. It is a fine-tuned version of DistilBERT, specifically trained on the Stanford Sentiment Treebank (SST-2) dataset for sentiment analysis. This model is optimized for classifying text as positive or negative sentiment, making it highly effective for analyzing opinions, reviews, and social media posts.</p>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">How to use?</h2>
        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 1: Import Required Libraries</h2>
        <p class="max-sm:text-lg text-xl">We start by importing the necessary modules. The `transformers` library gives us tools to use pre-trained models like DistilBERT, and `torch` helps us run the model.</p>
        <div class="mockup-code mt-2">
<pre><code>from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
  import torch  # PyTorch is used to work with the model</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 2: Load Tokenizer and Pre-trained Model</h2>
        <p class="max-sm:text-lg text-xl">The tokenizer turns text into tokens (numbers) that the model can understand. The model we load is already trained to detect sentiment.</p>
        <div class="mockup-code mt-2">
<pre><code>tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
  model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 3: Provide Input Text</h2>
        <p class="max-sm:text-lg text-xl">Here we define the sentence whose sentiment we want to analyze. You can change this to test different kinds of sentences.</p>
        <div class="mockup-code mt-2">
            <pre><code>text = "This product is terrible, I regret buying it."</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 4: Tokenize the Text</h2>
        <p class="max-sm:text-lg text-xl">The tokenizer converts the text into input IDs and attention masks â€” this prepares it for the model.</p>
        <div class="mockup-code mt-2">
            <pre><code>inputs = tokenizer(text, return_tensors="pt")  # Returns PyTorch tensors</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 5: Perform Sentiment Prediction</h2>
        <p class="max-sm:text-lg text-xl">We pass the tokenized input into the model. Using `torch.no_grad()` means we are only using the model, not training it.</p>
        <div class="mockup-code mt-2">
<pre><code>with torch.no_grad():
  outputs = model(**inputs)</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 6: Interpret the Results</h2>
        <p class="max-sm:text-lg text-xl">The model outputs 'logits', which are scores for each class. We use `argmax` to pick the class with the highest score.</p>
        <div class="mockup-code mt-2">
<pre><code>logits = outputs.logits
  predicted_class = torch.argmax(logits).item()</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Step 7: Map the Result to a Label</h2>
        <p class="max-sm:text-lg text-xl">We match the predicted class number to a human-readable label like POSITIVE or NEGATIVE and print it.</p>
        <div class="mockup-code mt-2">
<pre><code>labels = ["NEGATIVE", "POSITIVE"]
  print("Sentiment:", labels[predicted_class])</code></pre>
        </div>

        <p class="max-sm:text-lg text-xl mt-5">ðŸ’¡ <strong>Tip:</strong> Try changing the sentence in Step 3 to test other examples like:</p>
        <ul class="max-sm:text-lg text-xl list-disc list-inside">
            <li><code>"I love this movie, it was fantastic!"</code></li>
            <li><code>"The service was okay, nothing special."</code></li>
            <li><code>"Absolutely horrible experience. Never again."</code></li>
        </ul>
        <h2 class="max-sm:text-2xl text-3xl my-5 text-primary font-bold">Output</h2>
        <div class="mockup-code">
            <pre data-prefix="$"><code>NEGATIVE</code></pre>
        </div>

        <h2 class="max-sm:text-2xl text-3xl my-5 font-bold">Why <span class="text-primary font-bold font-poppins">Algolyzer</span> uses this model?</h2>
        <ul class="max-sm:text-lg text-xl">
            <li>âœ… Lightweight and Efficient - Faster than BERT while maintaining high accuracy.</li>
            <li>âœ… High Accuracy - Fine-tuned on SST-2, achieving state-of-the-art performance.</li>
            <li>âœ… Easy to Use - Direct integration with Hugging Faceâ€™s transformers library.</li>
            <li>âœ… Suitable for Real-time Applications - Can process text quickly, making it ideal for chatbots, feedback analysis, etc.</li>
        </ul>
    </div>
{% endblock %}